[2025-01-28 17:24:51,625] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2025-01-28 17:24:55,216] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 147, num_elems = 1.50B
[2025-01-28 17:24:56,298] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2025-01-28 17:24:56,385] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 294, num_elems = 3.00B
[2025-01-28 17:24:57,473] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown
[2025-01-28 17:24:57,474] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2025-01-28 17:24:57,479] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-01-28 17:24:57,480] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[2025-01-28 17:24:57,664] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-01-28 17:24:57,664] [INFO] [utils.py:782:see_memory_usage] MA 0.58 GB         Max_MA 2.04 GB         CA 2.53 GB         Max_CA 3 GB
[2025-01-28 17:24:57,665] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.83 GB, percent = 1.9%
Parameter Offload: Total persistent parameters: 67584 in 33 params
[2025-01-28 17:24:57,831] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-01-28 17:24:57,832] [INFO] [utils.py:782:see_memory_usage] MA 0.58 GB         Max_MA 0.58 GB         CA 2.53 GB         Max_CA 3 GB
[2025-01-28 17:24:57,832] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.83 GB, percent = 1.9%
[2025-01-28 17:24:57,833] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-01-28 17:24:57,833] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-01-28 17:24:57,833] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb23fefcee0>
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-01-28 17:24:57,834] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-01-28 17:24:57,835] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-28 17:24:57,838] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-28 17:24:57,838] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-01-28 17:24:57,838] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-01-28 17:24:57,838] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-01-28 17:24:57,838] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-01-28 17:24:57,838] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-01-28 17:24:57,838] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 6
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.01
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-28 17:24:57,839] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-01-28 17:24:57,840] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   train_batch_size ............. 48
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-01-28 17:24:57,841] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-01-28 17:24:57,842] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-01-28 17:24:57,842] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-01-28 17:24:57,842] [INFO] [config.py:1003:print]   world_size ................... 8
[2025-01-28 17:24:57,842] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-01-28 17:24:57,842] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-28 17:24:57,842] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-01-28 17:24:57,842] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-28 17:24:57,842] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-01-28 17:24:57,842] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 48,
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 6,
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "none",
            "nvme_path": null
        },
        "offload_param": {
            "device": "none",
            "nvme_path": null
        },
        "stage3_gather_16bit_weights_on_model_save": true
    },
    "gradient_clipping": 0.01,
    "steps_per_print": inf,
    "bf16": {
        "enabled": true
    },
    "fp16": {
        "enabled": false
    },
    "zero_optimization.reduce_bucket_size": 4.194304e+06,
    "zero_optimization.stage3_param_persistence_threshold": 2.048000e+04,
    "zero_optimization.stage3_prefetch_bucket_size": 3.774874e+06
}
Parameter Offload: Total persistent parameters: 67584 in 33 params
  0%|                                                                                                                              | 0/465 [00:00<?, ?it/s]
Invalidate trace cache @ step 0 and module 460: cache has only 0 modules
-------------------- Question:
Ahmed and Emily are having a contest to see who can get the best grade in the class. There have been 9 assignments and Ahmed has a 91 in the class. Emily has a 92. The final assignment is worth the same amount as all the other assignments. Emily got a 90 on the final assignment. What is the minimum grade Ahmed needs to get to beat Emily if all grades are whole numbers?
Answer:
100
Response:
To solve this, we'll first assume Emily's grade is the minimum grade she can have for the class. This is 90.

For Ahmed to beat Emily, his final grade must be higher than 90. Given that all other assignments were the same, the difference between the two can only come from the final assignment.

The difference between Ahmed's grade and Emily's grade on the final assignment is 2. Therefore, Ahmed must get at least 92 on the final assignment to beat Emily.
Extracted:
To solve this, we'll first assume Emily's grade is the minimum grade she can have for the class. This is 90.

For Ahmed to beat Emily, his final grade must be higher than 90. Given that all other assignments were the same, the difference between the two can only come from the final assignment.

The difference between Ahmed's grade and Emily's grade on the final assignment is 2. Therefore, Ahmed must get at least 92 on the final assignment to beat Emily.
Invalidate trace cache @ step 0 and module 690: cache has only 0 modules
-------------------- Question:
Marie has 98 unread messages on her phone. She decides to clear them by reading 20 messages a day. However, she also gets 6 new messages a day. How many days will it take her to read all her unread messages?
Answer:
7
Response:
<reasoning>
To find out how many days it will take, divide the number of unread messages by the number of messages read per day, then subtract the number of messages that arrive. To find this value first divide 98 by 20 and subtract 6, or 4.84 and round down to 4.84. So subtract the number of messages that arrive.
</reasoning>

<answer>
4.84 days
</answer>
Extracted:
4.84 days
  File "/home/ubuntu/R1_Project/grpo_demo.py", line 153, in <module>
    trainer.train()
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 3579, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 326, in compute_loss
    prompt_completion_ids = unwrapped_model.generate(**prompt_inputs, generation_config=self.generation_config)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/generation/utils.py", line 2215, in generate
    result = self._sample(
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/generation/utils.py", line 3206, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
    outputs = self.model(
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 945, in forward
    layer_outputs = decoder_layer(
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 676, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 561, in forward
    value_states = self.v_proj(hidden_states)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 116, in zero3_linear_wrap
    return LinearFunctionForZeroStage3.apply(input, weight)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 450, in decorate_fwd
    @functools.wraps(fwd)
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/R1_Project/grpo_demo.py", line 153, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 3579, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 326, in compute_loss
[rank0]:     prompt_completion_ids = unwrapped_model.generate(**prompt_inputs, generation_config=self.generation_config)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/generation/utils.py", line 2215, in generate
[rank0]:     result = self._sample(
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/generation/utils.py", line 3206, in _sample
[rank0]:     outputs = self(**model_inputs, return_dict=True)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
[rank0]:     outputs = self.model(
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 945, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 676, in forward
[rank0]:     hidden_states, self_attn_weights, present_key_value = self.self_attn(
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 561, in forward
[rank0]:     value_states = self.v_proj(hidden_states)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 116, in zero3_linear_wrap
[rank0]:     return LinearFunctionForZeroStage3.apply(input, weight)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 450, in decorate_fwd
[rank0]:     @functools.wraps(fwd)
[rank0]: KeyboardInterrupt
