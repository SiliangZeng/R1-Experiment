[2025-01-28 19:19:49,033] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-01-28 19:19:52,867] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[2025-01-28 19:19:55,215] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2025-01-28 19:19:55,395] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 678, num_elems = 3.55B
[2025-01-28 19:19:56,871] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown
[2025-01-28 19:19:56,871] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2025-01-28 19:19:56,881] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-01-28 19:19:56,883] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[2025-01-28 19:19:57,106] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-01-28 19:19:57,107] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 2.02 GB         CA 2.74 GB         Max_CA 3 GB
[2025-01-28 19:19:57,107] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 21.14 GB, percent = 1.9%
Parameter Offload: Total persistent parameters: 144896 in 141 params
[2025-01-28 19:19:57,311] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-01-28 19:19:57,312] [INFO] [utils.py:782:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 2.74 GB         Max_CA 3 GB
[2025-01-28 19:19:57,312] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 21.14 GB, percent = 1.9%
[2025-01-28 19:19:57,313] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-01-28 19:19:57,315] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-01-28 19:19:57,315] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-01-28 19:19:57,315] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-01-28 19:19:57,315] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-01-28 19:19:57,315] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-28 19:19:57,315] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-01-28 19:19:57,315] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9cb0cd5750>
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-01-28 19:19:57,316] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-01-28 19:19:57,317] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 4
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.1
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-01-28 19:19:57,318] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-01-28 19:19:57,319] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-01-28 19:19:57,320] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   train_batch_size ............. 32
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   world_size ................... 8
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-01-28 19:19:57,321] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-28 19:19:57,322] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-01-28 19:19:57,322] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-28 19:19:57,322] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-01-28 19:19:57,322] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 4,
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "none",
            "nvme_path": null
        },
        "offload_param": {
            "device": "none",
            "nvme_path": null
        },
        "stage3_gather_16bit_weights_on_model_save": true
    },
    "gradient_clipping": 0.1,
    "steps_per_print": inf,
    "bf16": {
        "enabled": true
    },
    "fp16": {
        "enabled": false
    },
    "zero_optimization.reduce_bucket_size": 2.359296e+06,
    "zero_optimization.stage3_param_persistence_threshold": 1.536000e+04,
    "zero_optimization.stage3_prefetch_bucket_size": 2.123366e+06
}
Parameter Offload: Total persistent parameters: 144896 in 141 params
  0%|                                                                                                          | 0/233 [00:00<?, ?it/s]Traceback (most recent call last):
Invalidate trace cache @ step 0 and module 796: cache has only 0 modules
  File "/home/ubuntu/R1_Project/code/R1-Experiment/grpo_demo_qwen.py", line 175, in <module>
    trainer.train()
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 3579, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 343, in compute_loss
    per_token_logps = get_per_token_logps(model, prompt_completion_ids)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 332, in get_per_token_logps
    logits = model(input_ids).logits  # (B, L, V)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1899, in forward
    loss = self.module(*inputs, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1164, in forward
    outputs = self.model(
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 895, in forward
    layer_outputs = decoder_layer(
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 638, in forward
    hidden_states = self.mlp(hidden_states)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 223, in forward
    return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 116, in zero3_linear_wrap
    return LinearFunctionForZeroStage3.apply(input, weight)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 455, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 64, in forward
    output = input.matmul(weight.t())
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 220.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 188.00 MiB is free. Including non-PyTorch memory, this process has 39.20 GiB memory in use. Of the allocated memory 37.58 GiB is allocated by PyTorch, and 131.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/R1_Project/code/R1-Experiment/grpo_demo_qwen.py", line 175, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/trainer.py", line 3579, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 343, in compute_loss
[rank0]:     per_token_logps = get_per_token_logps(model, prompt_completion_ids)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 332, in get_per_token_logps
[rank0]:     logits = model(input_ids).logits  # (B, L, V)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1899, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1164, in forward
[rank0]:     outputs = self.model(
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 895, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 638, in forward
[rank0]:     hidden_states = self.mlp(hidden_states)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 223, in forward
[rank0]:     return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1603, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 116, in zero3_linear_wrap
[rank0]:     return LinearFunctionForZeroStage3.apply(input, weight)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 455, in decorate_fwd
[rank0]:     return fwd(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/handbook/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 64, in forward
[rank0]:     output = input.matmul(weight.t())
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 220.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 188.00 MiB is free. Including non-PyTorch memory, this process has 39.20 GiB memory in use. Of the allocated memory 37.58 GiB is allocated by PyTorch, and 131.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
